[{"content":"Hello\n","date":"8 April 2024","externalUrl":null,"permalink":"/","section":"","summary":"Hello","title":"","type":"page"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/series/fabric-madness/","section":"Series","summary":"","title":"Fabric Madness","type":"series"},{"content":" Introduction # In our previous post we took a high level view of how to train a machine learning model in Microsoft Fabric. In this post we wanted to dive deeper into the process of feature engineering.\nFeature engineering is a crucial part of the development lifecycle for any Machine Learning (ML) systems. It is a step in the development cycle where raw data is processed to better represent its underlying structure and provide additional information that enhance our ML models. Feature engineering is both an art and a science. Even though there are specific steps that we can take to create good features, sometimes, it is only through experimentation that good results are achieved. Good features are crucial in guaranteeing a good system performance.\nAs datasets grow exponentially, traditional feature engineering may struggle with the size of very large datasets. This is where PySpark can help - as it is a scalable and efficient processing platform for massive datasets. A great thing about Fabric is that it makes using PySpark easy!\nIn this post, we\u0026rsquo;ll be going over:\nHow does PySpark Work? Basics of PySpark Feature Engineering in Action By the end of this post, hopefully you\u0026rsquo;ll feel comfortable carrying out feature engineering with PySpark in Fabric. Let\u0026rsquo;s get started!\nHow does PySpark Work? # Spark is a distributed computing system that allows for the processing of large datasets with speed and efficiency across a cluster of machines. It is built around the concept of a Resilient Distributed Dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. RDDs are the fundamental data structure of Spark, and they allow for the distribution of data across a cluster of machines.\nPySpark is the Python API for Spark. It allows for the creation of Spark DataFrames, which are similar to Pandas DataFrames, but with the added benefit of being distributed across a cluster of machines. PySpark DataFrames are the core data structure in PySpark, and they allow for the manipulation of large datasets in a distributed manner.\nAt the core of PySpark is the SparkSession object, which is what fundamentally interacts with Spark. This SparkSession is what allows for the creation of DataFrames, and other functionalities. Note that, when running a Notebook in Fabric, a SparkSession is automatically created for you, so you don\u0026rsquo;t have to worry about that.\nHaving a rough idea of how PySpark works, let\u0026rsquo;s get to the basics.\nBasics of PySpark # Although Spark DataFrames may remind us of Pandas DataFrames due to their similarities, the syntax when using PySpark can be a bit different. In this section, we\u0026rsquo;ll go over some of the basics of PySpark, such as reading data, combining DataFrames, selecting columns, grouping data, joining DataFrames, and using functions.\nThe Data # The data we are looking at is from the 2024 US college basketball tournaments, which was obtained from the on-going March Machine Learning Mania 2024 Kaggle competition, the details of which can be found here, and is licensed under CC BY 4.0\nReading data # As mentioned in the previous post of this series, the first step is usually to create a Lakehouse and upload some data. Then, when creating a Notebook, we can attach it to the created Lakehouse, and we\u0026rsquo;ll have access to the data stored there.\nPySpark Dataframes can read various data formats, such as CSV, JSON, Parquet, and others. Our data is stored in CSV format, so we\u0026rsquo;ll be using that, like in the following code snippet:\n# Read women\u0026#39;s data w_data = ( spark.read.option(\u0026#34;header\u0026#34;, True) .option(\u0026#34;inferSchema\u0026#34;, True) .csv(f\u0026#34;Files/WNCAATourneyDetailedResults.csv\u0026#34;) .cache() ) In this code snippet, we\u0026rsquo;re reading the detailed results data set of the final women\u0026rsquo;s basketball college tournament matches. Note that the \u0026quot;header\u0026quot; option being true means that the names of the columns will be derived from the first row of the CSV file. The inferSchema option tells Spark to guess the data types of the columns - otherwise they would all be read as strings. .cache() is used to keep the DataFrame in memory.\nIf you\u0026rsquo;re coming from Pandas, you may be wondering what the equivalent of df.head() is for PySpark - it\u0026rsquo;s df.show(5). The default for .show() is the top 20 rows, hence the need to specifically select 5.\nCombining DataFrames # Combining DataFrames can be done in multiple ways. The first we will look at is a union, where the columns are the same for both DataFrames:\n# Read women\u0026#39;s data ... # Read men\u0026#39;s data m_data = ( spark.read.option(\u0026#34;header\u0026#34;, True) .option(\u0026#34;inferSchema\u0026#34;, True) .csv(f\u0026#34;Files/MNCAATourneyDetailedResults.csv\u0026#34;) .cache() ) # Combine (union) the DataFrames combined_results = m_data.unionByName(w_data) Here, unionByName joins the two DataFrames by matching the names of the columns. Since both the women\u0026rsquo;s and the men\u0026rsquo;s detailed match results have the same columns, this is a good approach. Alternatively, there\u0026rsquo;s also union, which combines two Dataframes, matching column positions.\nSelecting Columns # Selecting columns from a DataFrame in PySpark can be done using the .select() method. We just have to indicate the name or names of the columns that are relevant as a parameter.\n# Selecting a single column w_scores = w_data.select(\u0026#34;WScore\u0026#34;) # Selecting multiple columns teamid_w_scores = w_data.select(\u0026#34;WTeamID\u0026#34;, \u0026#34;WScore\u0026#34;) Here\u0026rsquo;s the output for w_scores.show(5):\n+------+ |Season| +------+ | 2010| | 2010| | 2010| | 2010| | 2010| +------+ only showing top 5 rows The columns can also be renamed when being selected using the .alias() method:\nwinners = w_data.select( w_data.WTeamID.alias(\u0026#34;TeamID\u0026#34;), w_data.WScore.alias(\u0026#34;Score\u0026#34;)) Grouping Data # Grouping allows us to carry out certain operations for the groups that exist within the data and is usually combined with a aggregation functions. We can use .groupBy() for this:\n# Grouping and aggregating winners_average_scores = winners.groupBy(\u0026#34;TeamID\u0026#34;).avg(\u0026#34;Score\u0026#34;) In this example, we are grouping by \u0026quot;TeamID\u0026quot;, meaning we\u0026rsquo;re considering the groups of rows that have a distinct value for \u0026quot;TeamID\u0026quot;. For each of those groups, we\u0026rsquo;re calculating the average of the \u0026quot;Score\u0026quot;. This way, we get the average score for each team.\nHere\u0026rsquo;s the output of winners_average_scores.show(5), showing the average score of each team:\n+------+-----------------+ |TeamID| avg(Score)| +------+-----------------+ | 3125| 68.5| | 3345| 74.2| | 3346|79.66666666666667| | 3376|73.58333333333333| | 3107| 61.0| +------+-----------------+ Joining Data # Joining two DataFrames can be done using the .join() method. Joining is essentially extending the DataFrame by adding the columns of one DataFrame to another.\n# Joining on Season and TeamID final_df = matches_df.join(stats_df, on=[\u0026#39;Season\u0026#39;, \u0026#39;TeamID\u0026#39;], how=\u0026#39;left\u0026#39;) In this example, both stats_df and matches_df were using Season and TeamID as unique identifiers for each row. Besides Season and TeamID, stats_df has other columns, such as statistics for each team during each season, whereas matches_df has information about the matches, such as date and location. This operation allows us to add those interesting statistics to the matches information!\nFunctions # There are several functions that PySpark provides that help us transform DataFrames. You can find the full list here.\nHere\u0026rsquo;s an example of a simple function:\nfrom pyspark.sql import functions as F w_data = w_data.withColumn(\u0026#34;HighScore\u0026#34;, F.when(F.col(\u0026#34;Score\u0026#34;) \u0026gt; 80, \u0026#34;Yes\u0026#34;).otherwise(\u0026#34;No\u0026#34;)) In the code snippet above, a \u0026quot;HighScore\u0026quot; column is created when the score is higher than 80. For each row in the \u0026quot;Score\u0026quot; column (indicated by the .col() function), the value \u0026quot;Yes\u0026quot; is chosen for the \u0026quot;HighScore\u0026quot; column if the \u0026quot;Score\u0026quot; value is larger than 80, determined by the .when() function. .otherwise(), the value chosen is \u0026quot;No\u0026quot;.\nFeature Engineering in Action # Regular Season Statistics # Now that we have a basic understanding of PySpark and how it can be used, let\u0026rsquo;s go over how the regular season statistics features were created. These features were then used as inputs into our machine learning model to try to predict the outcome of the final tournament games.\nThe starting point was a DataFrame, regular_data, that contained match by match statistics for the regular seasons, which is the United States College Basketball Season that happens from November to March each year.\nEach row in this DataFrame contained the season, the day the match was held, the ID of team 1, the ID of team 2, and other information such as the location of the match. Importantly, it also contained statistics for each team for that specific match, such as \u0026quot;T1_FGM\u0026quot;, meaning the Field Goals Made (FGM) for team 1, or \u0026quot;T2_OR\u0026quot;, meaning the Offensive Rebounds (OR) of team 2.\nThe first step was selecting which columns would be used. These were columns that strictly contained in-game statistics.\n# Columns that we\u0026#39;ll want to get statistics from boxscore_cols = [ \u0026#39;T1_FGM\u0026#39;, \u0026#39;T1_FGA\u0026#39;, \u0026#39;T1_FGM3\u0026#39;, \u0026#39;T1_FGA3\u0026#39;, \u0026#39;T1_OR\u0026#39;, \u0026#39;T1_DR\u0026#39;, \u0026#39;T1_Ast\u0026#39;, \u0026#39;T1_Stl\u0026#39;, \u0026#39;T1_PF\u0026#39;, \u0026#39;T2_FGM\u0026#39;, \u0026#39;T2_FGA\u0026#39;, \u0026#39;T2_FGM3\u0026#39;, \u0026#39;T2_FGA3\u0026#39;, \u0026#39;T2_OR\u0026#39;, \u0026#39;T2_DR\u0026#39;, \u0026#39;T2_Ast\u0026#39;, \u0026#39;T2_Stl\u0026#39;, \u0026#39;T2_PF\u0026#39; ] If you\u0026rsquo;re interested, here\u0026rsquo;s what each statistic\u0026rsquo;s code means:\nFGM: Field Goals Made FGA: Field Goals Attempted FGM3: Field Goals Made from the 3-point-line FGA3: Field Goals Attempted for 3-point-line goals OR: Offensive Rebounds. A rebounds is when the ball rebounds from the board when a goal is attempted, not getting in the net. If the team that attempted the goal gets possession of the ball, it\u0026rsquo;s called an \u0026ldquo;Offensive\u0026rdquo; rebound. Otherwise, it\u0026rsquo;s called a \u0026ldquo;Defensive\u0026rdquo; Rebound. DR: Defensive Rebounds Ast: Assist, a pass that led directly to a goal Stl: Steal, when the possession of the ball is stolen PF: Personal Foul, when a player makes a foul From there, a dictionary of aggregation expressions was created. Basically, for each column name in the previous list of columns, a function was stored that would calculate the mean of the column, and rename it, by adding a suffix, \u0026quot;mean\u0026quot;.\nfrom pyspark.sql import functions as F from pyspark.sql.functions import col # select a column agg_exprs = {col: F.mean(col).alias(col + \u0026#39;mean\u0026#39;) for col in boxscore_cols} Then, the data was grouped by \u0026quot;Season\u0026quot; and \u0026quot;T1_TeamID\u0026quot;, and the aggregation functions of the previously created dictionary were used as the argument for .agg(). Note that (*agg_exprs.values()) is the same as (F.mean('T1_FGM').alias('T1_FGMmean'), F.mean('T1_FGA').alias('T1_FGAmean'), ..., F.mean('T2_PF').alias('T2_PFmean')).\nseason_statistics = regular_data.groupBy([\u0026#34;Season\u0026#34;, \u0026#34;T1_TeamID\u0026#34;]).agg(*agg_exprs.values()) Note that the grouping was done by season and the ID of team 1 - this means that \u0026quot;T2_FGAmean\u0026quot;, for example, will actually be the mean of the Field Goals Attempted made by the opponents of T1, not necessarily of a specific team. So, we actually need to rename the columns that are something like \u0026quot;T2_FGAmean\u0026quot; to something like \u0026quot;T1_opponent_FGAmean\u0026quot;.\n# Rename columns for T1 for col in boxscore_cols: season_statistics = season_statistics.withColumnRenamed(col + \u0026#39;mean\u0026#39;, \u0026#39;T1_\u0026#39; + col[3:] + \u0026#39;mean\u0026#39;) if \u0026#39;T1_\u0026#39; in col \\ else season_statistics.withColumnRenamed(col + \u0026#39;mean\u0026#39;, \u0026#39;T1_opponent_\u0026#39; + col[3:] + \u0026#39;mean\u0026#39;) At this point, it\u0026rsquo;s important to mention that the regular_data DataFrame actually has two rows per each match that occurred. This is so that both teams can be \u0026ldquo;T1\u0026rdquo; and \u0026ldquo;T2\u0026rdquo;, for each match. This little \u0026ldquo;trick\u0026rdquo; is what makes these statistics useful.\nNote that we \u0026ldquo;only\u0026rdquo; have the statistics for \u0026ldquo;T1\u0026rdquo;. We \u0026ldquo;need\u0026rdquo; the statistics for \u0026ldquo;T2\u0026rdquo; as well - \u0026ldquo;need\u0026rdquo; in quotations because there are no new statistics being calculated. We just need the same data, but with the columns having different names, so that for a match with \u0026ldquo;T1\u0026rdquo; and \u0026ldquo;T2\u0026rdquo;, we have statistics for both T1 and T2. So, we created a mirror DataFrame, where, instead of \u0026ldquo;T1\u0026hellip;mean\u0026rdquo; and \u0026ldquo;T1_opponent_\u0026hellip;mean\u0026rdquo;, we have \u0026ldquo;T2\u0026hellip;mean\u0026rdquo; and \u0026ldquo;T2_opponent_\u0026hellip;mean\u0026rdquo;. This is important because, later on, when we\u0026rsquo;re joining these regular season statistics to tournament matches, we\u0026rsquo;ll be able to have statistics for both team 1 and team 2.\nseason_statistics_T2 = season_statistics.select( *[F.col(col).alias(col.replace(\u0026#39;T1_opponent_\u0026#39;, \u0026#39;T2_opponent_\u0026#39;).replace(\u0026#39;T1_\u0026#39;, \u0026#39;T2_\u0026#39;)) if col not in [\u0026#39;Season\u0026#39;] else F.col(col) for col in season_statistics.columns] ) Now, there are two DataFrames, with season statistics for \u0026ldquo;both\u0026rdquo; T1 and T2. Since the final dataframe will contain the \u0026ldquo;Season\u0026rdquo;, the \u0026ldquo;T1TeamID\u0026rdquo; and the \u0026ldquo;T2TeamID\u0026rdquo;, we can join these newly created features with a join!\ntourney_df = tourney_df.join(season_statistics, on=[\u0026#39;Season\u0026#39;, \u0026#39;T1_TeamID\u0026#39;], how=\u0026#39;left\u0026#39;) tourney_df = tourney_df.join(season_statistics_T2, on=[\u0026#39;Season\u0026#39;, \u0026#39;T2_TeamID\u0026#39;], how=\u0026#39;left\u0026#39;) Elo Ratings # First created by Arpad Elo, Elo is a rating system for zero-sum games (games where one player wins and the other loses), like basketball. With the Elo rating system, each team has an Elo rating, a value that generally conveys the team\u0026rsquo;s quality. At first, every team has the same Elo, and whenever they win, their Elo increases, and when they lose, their Elo decreases. A key characteristic of this system is that this value increases more with a win against a strong opponent than with a win against a weak opponent. Thus, it can be a very useful feature to have!\nWe wanted to capture the Elo rating of a team at the end of the regular season, and use that as feature for the tournament. To do this, we calculated the Elo for each team on a per match basis. To calculate Elo for this feature, we found it more straightforward to use Pandas.\nCentral to Elo is calculating the expected score for each team. It can be described in code like so:\n# Function to calculate expected score def expected_score(ra, rb): # ra = rating (Elo) team A # rb = rating (Elo) team B # Elo function return 1 / (1 + 10 ** ((rb - ra) / 400)) Considering a team A and a team B, this function computes the expected score of team A against team B.\nFor each match, we would update the teams\u0026rsquo; Elos. Note that the location of the match also played a part - winning at home was considered less impressive than winning away.\n# Function to update Elo ratings, keeping T1 and T2 terminology def update_elo(t1_elo, t2_elo, location, T1_Score, T2_Score): expected_t1 = expected_score(t1_elo, t2_elo) expected_t2 = expected_score(t2_elo, t1_elo) actual_t1 = 1 if T1_Score \u0026gt; T2_Score else 0 actual_t2 = 1 - actual_t1 # Determine K based on game location # The larger the K, the bigger the impact # team1 winning at home (location=1) less impressive than winning away (location = -1) if actual_t1 == 1: # team1 won if location == 1: k = 20 elif location == 0: k = 30 else: # location = -1 k = 40 else: # team2 won if location == 1: k = 40 elif location == 0: k = 30 else: # location = -1 k = 20 new_t1_elo = t1_elo + k * (actual_t1 - expected_t1) new_t2_elo = t2_elo + k * (actual_t2 - expected_t2) return new_t1_elo, new_t2_elo To apply the Elo rating system, we iterated through each season\u0026rsquo;s matches, initializing teams with a base rating and updating their ratings match by match. The final Elo available for each team in each season will, hopefully, be a good descriptor of the team\u0026rsquo;s quality.\ndef calculate_elo_through_seasons(regular_data): # For this feature, using Pandas regular_data = regular_data.toPandas() # Set value of initial elo initial_elo = 1500 # DataFrame to collect final Elo ratings final_elo_list = [] for season in sorted(regular_data[\u0026#39;Season\u0026#39;].unique()): print(f\u0026#34;Season: {season}\u0026#34;) # Initialize elo ratings dictionary elo_ratings = {} print(f\u0026#34;Processing Season: {season}\u0026#34;) # Get the teams that played in the season season_teams = set(regular_data[regular_data[\u0026#39;Season\u0026#39;] == season][\u0026#39;T1_TeamID\u0026#39;]).union(set(regular_data[regular_data[\u0026#39;Season\u0026#39;] == season][\u0026#39;T2_TeamID\u0026#39;])) # Initialize season teams\u0026#39; Elo ratings for team in season_teams: if (season, team) not in elo_ratings: elo_ratings[(season, team)] = initial_elo # Update Elo ratings per game season_games = regular_data[regular_data[\u0026#39;Season\u0026#39;] == season] for _, row in season_games.iterrows(): t1_elo = elo_ratings[(season, row[\u0026#39;T1_TeamID\u0026#39;])] t2_elo = elo_ratings[(season, row[\u0026#39;T2_TeamID\u0026#39;])] new_t1_elo, new_t2_elo = update_elo(t1_elo, t2_elo, row[\u0026#39;location\u0026#39;], row[\u0026#39;T1_Score\u0026#39;], row[\u0026#39;T2_Score\u0026#39;]) # Only keep the last season rating elo_ratings[(season, row[\u0026#39;T1_TeamID\u0026#39;])] = new_t1_elo elo_ratings[(season, row[\u0026#39;T2_TeamID\u0026#39;])] = new_t2_elo # Collect final Elo ratings for the season for team in season_teams: final_elo_list.append({\u0026#39;Season\u0026#39;: season, \u0026#39;TeamID\u0026#39;: team, \u0026#39;Elo\u0026#39;: elo_ratings[(season, team)]}) # Convert list to DataFrame final_elo_df = pd.DataFrame(final_elo_list) # Separate DataFrames for T1 and T2 final_elo_t1_df = final_elo_df.copy().rename(columns={\u0026#39;TeamID\u0026#39;: \u0026#39;T1_TeamID\u0026#39;, \u0026#39;Elo\u0026#39;: \u0026#39;T1_Elo\u0026#39;}) final_elo_t2_df = final_elo_df.copy().rename(columns={\u0026#39;TeamID\u0026#39;: \u0026#39;T2_TeamID\u0026#39;, \u0026#39;Elo\u0026#39;: \u0026#39;T2_Elo\u0026#39;}) # Convert the pandas DataFrames back to Spark DataFrames final_elo_t1_df = spark.createDataFrame(final_elo_t1_df) final_elo_t2_df = spark.createDataFrame(final_elo_t2_df) return final_elo_t1_df, final_elo_t2_df Ideally, we wouldn\u0026rsquo;t calculate Elo changes on a match-by-match basis to determine each team\u0026rsquo;s final Elo for the season. However, we couldn\u0026rsquo;t come up with a better approach. Do you have any ideas? If so, let us know!\nValue Added # The feature engineering steps demonstrated show how we can transform raw data - regular season statistics - into valuable information with predictive power. It is reasonable to assume that a team\u0026rsquo;s performance during the regular season is indicative of its potential performance in the final tournaments. By calculating the mean of observed match-by-match statistics for both the teams and their opponents, along with each team\u0026rsquo;s Elo rating in their final match, we were able to create a dataset suitable for modelling. Then, models were trained to predict the outcome of tournament matches using these features, among others developed in a similar way. With these models, we only need the two team IDs to look up the mean of their regular season statistics and their Elos to feed into the model and predict a score!\nConclusion # In this post, we looked at some of the theory behind Spark and PySpark, how that can be applied, and a concrete practical example. We explored how feature engineering can be done in the case of sports data, creating regular season statistics to use as features for final tournament games. Hopefully you\u0026rsquo;ve found this interesting and helpful - happy feature engineering!\n","date":"8 April 2024","externalUrl":null,"permalink":"/posts/fabric-madness-2/","section":"Blog","summary":"In part 2 of this series we dive deeper into the process of feature engineering.","title":"Fabric Madness: Feature Engineering with pyspark","type":"posts"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/authors/martimchaves/","section":"Authors","summary":"","title":"Martimchaves","type":"authors"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/tags/microsoft-fabric/","section":"Tags","summary":"","title":"Microsoft Fabric","type":"tags"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/tags/spark/","section":"Tags","summary":"","title":"Spark","type":"tags"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Introduction # At the time of writing, it\u0026rsquo;s basketball season in the United States, and there is a lot of excitement around the men\u0026rsquo;s and women\u0026rsquo;s college basketball tournaments. The format is single elimination, so over the course of several rounds, teams are eliminated, till eventually we get a champion. This tournament is not only a showcase of upcoming basketball talent, but, more importantly, a fertile ground for data enthusiasts like us to analyze trends and predict outcomes.\nOne of the great things about sports is that there is lots of data available, and we at Noble Dynamic wanted to take a crack at it ü§ì.\nIn this series of posts titled Fabric Madness, we\u0026rsquo;re going to be diving deep into some of the most interesting features of Microsoft Fabric, for an end-to-end demonstration of how to train and use a machine learning model.\nIn this first blog post, we\u0026rsquo;ll be going over:\nA first look at the data using Data Wrangler. Exploratory Data Analysis (EDA) and Feature Engineering Tracking the performance of different Machine Learning (ML) Models using Experiments Selecting the best performing model using the ML Model functionality The Data # The data used was obtained from the on-going Kaggle competition, the details of which can be found here.\nAmong all of the interesting data available, our focus for this case study was on the match-by-match statistics. This data was available for both the regular seasons and the tournaments, going all the way back to 2003. For each match, besides the date, the teams that were playing, and their scores, other relevant features were made available, such as field goals made and personal fouls by each team.\nLoading the Data # The first step was creating a Fabric Workspace. Workspaces in Fabric are one of the fundamental building blocks of the platform, and are used for grouping together related items and for collaboration.\nAfter downloading all of the CSV files available, a Lakehouse was created. A Lakehouse, in simple terms, is a mix between a Database of Tables (structured) and a Data Lake of Files (unstructured). The big benefit of a Lakehouse is that data is available for every tool in the workspace.\nUploading the files was done using the UI:\nFig. 1 - Uploading Files Now that we have a Lakehouse with the CSV files, it was time to dig in, and get a first look at the data. To do that, we created a Notebook, using the UI, and attached the previously created Lakehouse.\nFig. 2 - Adding Lakehouse to Notebook First Look # After a quick data wrangling, it was found that, as expected with data from Kaggle, the quality was great. With no duplicates or missing values.\nFor this task we used Data Wrangler, a tool built into Microsoft Fabric notebooks. Once an initial DataFrame has been created (Spark or Pandas supported), Data Wrangler becomes available to use and can attach to any DataFrame in the Notebook. What\u0026rsquo;s great is that it allows for easy analysis of loaded DataFrames.\nIn a Notebook, after reading the files into PySpark DataFrames, in the \u0026ldquo;Data\u0026rdquo; section, the \u0026ldquo;Transform DataFrame in Data Wrangler\u0026rdquo; was selected, and from there the several DataFrames were explored. Specific DataFrames can be chosen, carrying out a careful inspection.\nFig. 3 - Opening Data Wrangler Fig. 4 - Analysing the DataFrame with Data Wrangler In the centre, we have access to all of the rows of the loaded DataFrame. On the right, a Summary tab, showing that indeed there are no duplicates or missing values. Clicking in a certain column, summary statistics of that column will be shown.\nOn the left, in the Operations tab, there are several pre-built operations that can be applied to the DataFrame. The operations feature many of the most common data wrangling tasks, such as filtering, sorting, and grouping, and is a quick way to generate boilerplate code for these tasks.\nIn our case, the data was already in good shape, so we moved on to the EDA stage.\nExploratory Data Analysis # A short Exploratory Data Analysis (EDA) followed, with the goal of getting a general idea of the data. Charts were plotted to get a sense of the distribution of the data and if there were any statistics that could be problematic due to, for example, very long tails.\nFig. 5 - Histogram of field goals made At a quick glance, it was found that the data available from the regular season had normal distributions, suitable to use in the creation of features. Knowing the importance that good features have in creating solid predictive systems, the next sensible step was to carry out feature engineering to extract relevant information from the data.\nThe goal was to create a dataset where each sample\u0026rsquo;s input would be a set of features for a game, containing information of both teams. For example, both teams average field goals made for the regular season. The target for each sample, the desired output, would be 1 if Team 1 won the game, or 0 if Team 2 won the game (which was done by subtracting the scores). Here\u0026rsquo;s a representation of the dataset:\nTeam1ID Team2ID Team1Feat1 Team2Feat2 T1Score-T2Score Target 1 2 0.5 0.6 8 1 3 1 0.2 0.7 12 1 2 4 0.8 0.6 -3 0 Feature Engineering # The first feature that we decided to explore was win rate. Not only would it be an interesting feature to explore, but it would also provide a baseline score. This initial approach employed a simple rule: the team with the higher win rate would be predicted as the winner. This method provides a fundamental baseline against which the performance of more sophisticated predictive systems can be compared to.\nTo evaluate the accuracy of our predictions across different models, we adopted the Brier score. The Brier score is the mean of the square of the difference between the predicted probability (p) and the actual outcome (o) for each sample, and can be described by the following formula:\n\\(\\Large Brier Score = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - o_i)^2 \\)\nThe predicted probability will vary between 0 and 1, and the actual outcome will either be 0 or 1. Thus, the Brier score will always be between 0 and 1. As we want the predicted probability to be as close to the actual outcome as possible, the lower the Brier score, the better, with 0 being the perfect score, and 1 the worst.\nFor the baseline, the previously mentioned dataset structure was followed. Each sample of the dataset was a match, containing the win rates for the regular season for Team 1 and Team 2. The actual outcome was considered 1 if Team 1 won, or 0 if Team 2 won. To simulate a probability, the prediction was a normalised difference between T1\u0026rsquo;s win rate and T2\u0026rsquo;s win rate. For the maximum value of the difference between the win rates, the prediction would be 1. For the minimum value, the prediction would be 0.\n# Add the \u0026#34;outcome\u0026#34; column: 1 if T1_Score \u0026gt; T2_Score, else 0 tourney_df = tourney_df.withColumn(\u0026#34;outcome\u0026#34;, F.when(F.col(\u0026#34;T1_Score\u0026#34;) \u0026gt; F.col(\u0026#34;T2_Score\u0026#34;), 1).otherwise(0)) # Adjust range from [-1, 1] to [0, 1]. If below .5 T1 loses, if above .5 T1 wins. If same win rate, assumed \u0026#34;draw\u0026#34; tourney_df = tourney_df.withColumn(\u0026#34;probability\u0026#34;, (F.col(\u0026#34;T1_win_ratio\u0026#34;) - F.col(\u0026#34;T2_win_ratio\u0026#34;) + 1) / 2) After calculating the win rate, and then using it to predict the outcomes, we got a Brier score of 0.23. Considering that guessing at random leads to a Brier score of 0.25, it\u0026rsquo;s clear that this feature alone is not very good üò¨.\nBy starting with a simple baseline, it clearly highlighted that more complex patterns were at play. We went ahead to developed another 42 features, in preparation for utilising more complex algorithms, machine learning models, that might have a better chance.\nIt was then time to create machine learning models!\nModels \u0026amp; Machine Learning Experiments # For the models, we opted for simple Neural Networks (NN). To determine which level of complexity would be best, we created three different NNs, with an increasing number of layers and hyper-parameters. Here\u0026rsquo;s an example of a small NN, one that was used:\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense def create_small_NN(input_shape): model = Sequential([ Dense(64, activation=\u0026#39;relu\u0026#39;, input_shape=(number_of_features,)), Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) return model Fig. 6 - Diagram of a Neural Network If you\u0026rsquo;re familiar with NNs, feel free to skip to the Experiments! If you\u0026rsquo;re unfamiliar with NNs think of them as a set of layers, where each layer acts as a filter for relevant information. Data passes through successive layers, in a step-by-step fashion, where each layer has inputs and outputs. Data moves through the network in one direction, from the first layer (the model\u0026rsquo;s input) to the last layer (the model\u0026rsquo;s output), without looping back, hence the Sequential function.\nEach layer is made up of several neurons, that can be described as nodes. The model\u0026rsquo;s input, the first layer, will contain as many neurons as there are features available, and each neuron will hold the value of a feature. The model\u0026rsquo;s output, the last layer, in binary problems such as the one we\u0026rsquo;re tackling, will only have 1 neuron. The value held by this neuron should be 1 if the model is processing a match where Team 1 won, or 0 if Team 2 won. The intermediate layers have an ad hoc number of neurons. In the example in the code snippet, 64 neurons were chosen.\nIn a Dense layer, as is the case here, each neuron in the layer is connected to every neuron in the preceding layer. Fundamentally, each neuron processes the information provided by the neurons from the previous layer.\nThe processing of the previous layer\u0026rsquo;s information requires an activation function. There are many types of activation functions - ReLU, standing for Rectified Linear Unit, is one of them. It allows only positive values to pass and sets negative values to zero, making it effective for many types of data.\nNote that the final activation function is a sigmoid function - this converts the output to a number between 0 and 1. This is crucial for binary classification tasks, where you need the model to express its output as a probability.\nBesides these small models, medium and large models were created, with an increasing number of layers and parameters. The size of a model affects its ability to capture complex patterns in the data, with larger models generally being more capable in this regard. However, larger models also require more data to learn effectively - if there\u0026rsquo;s not enough data, issues may occur. Finding the right size is sometimes only possible through experimentation, by training different models and comparing their performance to identify the most effective configuration.\nThe next step was running the experiments ‚öóÔ∏è!\nWhat is an Experiment? # In Fabric, an Experiment can be seen as a group of related runs, where a run is an execution of a code snippet. In this context, a run is a training of a model. For each run, a model will be trained with a different set of hyper-parameters. The set of hyper-parameters, along with the final model score, is logged, and this information is available for each run. Once enough runs have been completed, the final model scores can be compared, so that the best version of each model can be selected.\nCreating an Experiment in Fabric can be done via the UI or directly from a Notebook. The Experiment is essentially a wrapper for MLFlow Experiments. One of the great things about using Experiments in Fabric is that the results can be shared with others. This makes it possible to collaborate and allow others to participate in experiments, either writing code to run experiments, or analysing the results.\nCreating an Experiment # Using the UI to create an Experiment simply select Experiment from the + New button, and choose a name.\nFig. 7 - Creating an Experiment using the UI When training each of the models, the hyper-parameters are logged with the experiment, as well as the final score. Once completed we can see the results in the UI, and compare the different runs to see which model performed best.\nFig. 8 - Comparing different runs After that we can select the best model and use it to make the final prediction. When comparing the three models, the best Brier score was 0.20, a slight improvement üéâ!\nConclusion # After loading and analysing data from this year\u0026rsquo;s US major college basketball tournament, and creating a dataset with relevant features, we were able to predict the outcome of the games using a simple Neural Network. Experiments were used to compare the performance of different models. Finally, the best performing model was selected to carry out the final prediction.\nIn the next post we will go into detail on how we created the features using pyspark. Stay tuned for more! üëã\nThe full source code for this post can be found here.\n","date":"1 April 2024","externalUrl":null,"permalink":"/posts/fabric-madness-1/","section":"Blog","summary":"In this series of posts titled Fabric Madness, we\u0026rsquo;re going to be diving deep into some of the most interesting features of Microsoft Fabric, for an end-to-end demonstration of how to train and use a machine learning model.","title":"Fabric Madness: predicting basketball games with Microsoft Fabric","type":"posts"},{"content":"","date":"1 April 2024","externalUrl":null,"permalink":"/authors/rogernoble/","section":"Authors","summary":"","title":"Rogernoble","type":"authors"},{"content":"","date":"26 March 2024","externalUrl":null,"permalink":"/posts/","section":"Blog","summary":"","title":"Blog","type":"posts"},{"content":" Website # The information contained in this website is for general information purposes only. The information is provided by Noble Dynamic Limited and while we endeavour to keep the information up to date and correct, we make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the website or the information, products, services, or related graphics contained on the website for any purpose.\nAny reliance you place on such information is therefore strictly at your own risk.\nIn no event will we be liable for any loss or damage including without limitation, indirect or consequential loss or damage, or any loss or damage whatsoever arising from loss of data or profits arising out of, or in connection with, the use of this website.\nEmails # Emails and any files or attachments sent by Noble Dynamic are confidential and solely for the use of the individual or organisation to whom they are addressed.\nIf you have received and email in error, please immediately notify the sender. Any disclosure, copying or distribution of our emails, including attachments, is strictly prohibited. Any views or opinions presented do not necessarily represent those of the company.\nWarning: Computer viruses can be transmitted via email. The company takes reasonable precautions to ensure no viruses are present but accepts no liability for any loss or damage arising from the use of our emails or attachments.\n","date":"16 August 2022","externalUrl":null,"permalink":"/disclaimer/","section":"","summary":"Website # The information contained in this website is for general information purposes only.","title":"Disclaimer","type":"page"},{"content":"By using this website, you agree to comply with, and be bound by, the following terms and conditions of use, which together with our privacy policy govern Noble Dynamic Limited‚Äôs relationship with you in relation to this website. If you disagree with any part of these terms and conditions, please do not use our website.\nThe term \u0026lsquo;Noble Dynamic Limited\u0026rsquo; or \u0026lsquo;Noble Dynamic\u0026rsquo; or \u0026lsquo;us\u0026rsquo; or \u0026lsquo;we\u0026rsquo; refers to Noble Dynamic Limited incorporated and registered in England and Wales with company number 14292543 whose registered office is at: 86-90 Paul Street, London EC2A 4NE\nThe term \u0026lsquo;you\u0026rsquo; refers to the user or viewer of our website. The use of this website is subject to the following terms of use:\nThe content of the pages of this website is for your general information and use only. It is subject to change without notice.\nThis website does not use cookies to monitor browsing preferences.\nNeither we, nor any third parties, provide any warranty or guarantee as to the accuracy, timeliness, performance, completeness or suitability of the information and materials found or offered on this website for any particular purpose.\nYou acknowledge that such information and materials may contain inaccuracies or errors and we expressly exclude liability for any such inaccuracies or errors to the fullest extent permitted by law.\nYour use of any information or materials on this website is entirely at your own risk, for which we shall not be liable. It shall be your own responsibility to ensure that any products, services or information available through this website meet your specific requirements.\nThis website contains material which is owned by or licensed to us. This material includes, but is not limited to, the design, layout, look, appearance and graphics.\nReproduction is prohibited other than in accordance with the copyright notice, which forms part of these terms and conditions.\nAll trademarks reproduced in this website, which are not the property of, or licensed to, the operator, are acknowledged on the website.\nUnauthorised use of this website may give rise to a claim for damages and/or be a criminal offence.\nFrom time to time, this website may also include links to other websites. These links are provided for your convenience to provide further information. They do not signify that we endorse the website(s). We have no responsibility for the content of the linked website(s).\nThese terms and conditions are governed by the law of England and Wales. You agree to submit to the non-exclusive jurisdiction of the courts of England and Wales in relation to any disputes arising under or in connection with these terms and conditions or the contract between us.\n","date":"16 August 2022","externalUrl":null,"permalink":"/terms-conditions/","section":"","summary":"By using this website, you agree to comply with, and be bound by, the following terms and conditions of use, which together with our privacy policy govern Noble Dynamic Limited‚Äôs relationship with you in relation to this website.","title":"Terms and Conditions","type":"page"},{"content":" Your Privacy on this Website # Noble Dynamic is committed to protecting users‚Äô privacy. This includes nobledynamic.com and associated apps (this \u0026ldquo;Site\u0026rdquo;). We offer this statement to inform Our users of how We define, gather and use (\u0026quot;Personal Information\u0026quot;). This is submitted to Us via this Site. We will take reasonable steps to protect Personal Information submitted by users. Protection in such a way as is consistent with this Privacy Statement. Also in accordance with all applicable data protection and privacy laws. The Personal Information that We collect from users may include the following: a person‚Äôs full name, address, telephone number and/ or email address.\nIf you do not wish Us to process your Personal Information for the reasons and in the manner set out in this Privacy Statement, please do not use this Site or any of the services available through it.\nOur collection of personally identifiable information # We gather information on our users in two different ways:\nYou supply it in response to a request from Us. Cookie technology. We collect Personal Information voluntarily submitted by its users as part of the registration process for its services. Competition participation will require disclosing Personal Information.\nOur use of personally identifiable information # We may use Personal Information collected online in the following ways:\nto develop and improve the products, benefits and services We or carefully selected business partners offer to Our registered users and other users of the Site; to enable users to become registered users of the Site and to interact with online products and services; for Site administration and development (for example to analyse usage trends and to make improvements to the Site in accordance with such usage trends); to contact you by post and email, or by SMS, to inform you about any of Our products and services (including those of selected business partners) that We think may be of interest. We will not do this without having your prior permission. If at any time you subsequently wish to discontinue receiving post or emails about Our products and services please indicate this by responding. If you no longer wish to receive promotional communications by SMS, you will be given the option to opt out. From time to time we may also wish to contact you by telephone. In the event that we do so we shall always respect any request you make to decline such telephone marketing in the future; to contact you, where you have authorised Us to do so or to provide you with a email newsletter; and in assessing your request for goods or services for the purposes of the prevention and detection of fraud. In the event that We undergo a re-organisation, we may transfer your Personal Information. This transfer would be to the new owner of the Site if you are a registered user.\nDisclosure of personally identifiable information # We will not sell, rent or disclose your Personal Information to third parties without permission. Hence, you agree that We may without your prior permission disclose such information to other Noble Dynamic related companies. Or our suppliers, subcontractors and business partners where necessary. This in order to operate this Site and/or to provide you with services that you have requested.\nTherefore, you agree that We may without your prior permission disclose your Personal Information to other Noble Dynamic companies. And to our suppliers, subcontractors and business partners where necessary. This in order to operate this Site and/or to provide you with services that you have requested. We may transfer your Personal Information to other Noble Dynamic companies. And to our suppliers, subcontractors, business partners located outside of the European Economic Area (\u0026quot;EEA\u0026quot;). You should be aware that some of these countries might have data protection laws. These are equivalent to the Data Protection Act 1998. It regulates the use of personal data by companies and other business in the UK. However, some may not. You hereby consent to Our transferring your Personal Information to other Noble Dynamic companies. Or our suppliers, sub-contractors and business partners located outside the EEA. All of this in connection with your use of this Site.\nHence, Personal Information may also be disclosed to third party organisations. This may be done without your prior permission where there is a legal obligation on Us to do so.\nData Protection Act 1998 # We comply with the Data Protection Act of 1998 (the \u0026ldquo;Act\u0026rdquo;). Also with all other applicable UK data protection and privacy legislation.\nSecurity of Your Information # Therefore, we take the security of your Personal Information seriously. In addition, we take all reasonable precautions to prevent the loss, misuse or alteration of it. Agents, suppliers, subcontractors, business partners have access to your Personal Information. Confidentiality is required. They are not permitted to use it for any purpose. None other than to carry out the provision of services which they are providing to Us. Or otherwise as set out in this Privacy Statement.\nWe have a secure server that uses Secure Socket Layer (SSL) encryption to protect your Personal Information.\nContact Us # Finally, if you have any questions concerning Our Privacy Statement, you can contact our team by:\nEmail: info@nobledynamic.com\nAmendments to this privacy statement # In conclusion, we reserve the right to modify or update this Privacy Statement at any time. This site publishes any amendments. Privacy Statement review encouraged. After publish of an amended Privacy Statement, consent deemed by continuing to use this Site. If you do not agree to the amendments, please do not continue to use this Site.\n","date":"16 August 2022","externalUrl":null,"permalink":"/privacy-policy/","section":"","summary":"Your Privacy on this Website # Noble Dynamic is committed to protecting users‚Äô privacy.","title":"Privacy Policy","type":"page"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]